{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXaFSkUu0fzm"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?export=view&id=1UXScsVx_Wni_JuDdB8LeTnM6jsPfIwkW)\n",
    "\n",
    "Proprietary content. Â© Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Project - Seq NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OudB5by50jlI"
   },
   "source": [
    "# Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Description:\n",
    "Generate Word Embedding and retrieve outputs of each layer with Keras based on the Classification task.\n",
    "\n",
    "Word embedding are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "\n",
    "It is a distributed representation for the text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
    "\n",
    "We will use the IMDb dataset to learn word embedding as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with a sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "### Dataset\n",
    "- Dataset of 25,000 movie reviews from IMDB, labeled by sentiment positive (1) or negative (0)\n",
    "- Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers).\n",
    "- For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "- As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "Command to import data\n",
    "- `from tensorflow.keras.datasets import imdb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from itertools import islice\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D, Flatten, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "tf.random.set_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mounting Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# Setting the current working directory\n",
    "#import os; os.chdir('drive/My Drive/Great Learning/NLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q34-Y3nRKXdO"
   },
   "source": [
    "### Import the data (4 Marks)\n",
    "- Use `imdb.load_data()` method\n",
    "- Get train and test set\n",
    "- Take 10000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxfwbrbuKbk2"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import test and train data (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading Dataset - Train & Test Split\n",
    "vocab_size = 10000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "imdb_data=imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Review length: 238.71364\n",
      "Standard Deviation: 176\n"
     ]
    }
   ],
   "source": [
    "length = [len(i) for i in x_train]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DldivBO4LTbP"
   },
   "source": [
    "### Pad each sentence to be of same length (4 Marks)\n",
    "- Take maximum sequence length as 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E808XB4tLtic"
   },
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "x_train = pad_sequences(x_train, maxlen = maxlen, padding = 'pre')\n",
    "x_test =  pad_sequences(x_test, maxlen = maxlen, padding = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the labels (train and test) (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((x_train, x_test), axis = 0)\n",
    "y = np.concatenate((y_train, y_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state, shuffle = True)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size = 0.2, random_state = random_state, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBFFCrybMSXz"
   },
   "source": [
    "### Print shape of features & labels (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOcyRtZfMYZd"
   },
   "source": [
    "Number of review, number of words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdMCUPr7RaCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number reviews: 50000\n",
      "Number of words in each review: 300\n",
      "Number of unique words: 9999\n"
     ]
    }
   ],
   "source": [
    "print( 'Number reviews:', X.shape[0])\n",
    "print( 'Number of words in each review:', X.shape[1])\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cNk5sDvMr3j"
   },
   "source": [
    "Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number lablels 50000\n"
     ]
    }
   ],
   "source": [
    "print( 'Number lablels', y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lablels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print( 'Number of unique lablels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------ \n",
      "Number of rows in training dataset: 32000\n",
      "Number of columns in training dataset: 300\n",
      "Number of unique words in training dataset: 9999\n",
      "------------------------------------------------------------ \n",
      "Number of rows in validation dataset: 8000\n",
      "Number of columns in validation dataset: 300\n",
      "Number of unique words in validation dataset: 9984\n",
      "------------------------------------------------------------ \n",
      "Number of rows in test dataset: 10000\n",
      "Number of columns in test dataset: 300\n",
      "Number of unique words in test dataset: 9995\n",
      "------------------------------------------------------------ \n",
      "Unique Categories: (array([0, 1], dtype=int64), array([0, 1], dtype=int64), array([0, 1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print('---'*20, f'\\nNumber of rows in training dataset: {x_train.shape[0]}')\n",
    "print(f'Number of columns in training dataset: {x_train.shape[1]}')\n",
    "print(f'Number of unique words in training dataset: {len(np.unique(np.hstack(x_train)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nNumber of rows in validation dataset: {x_valid.shape[0]}')\n",
    "print(f'Number of columns in validation dataset: {x_valid.shape[1]}')\n",
    "print(f'Number of unique words in validation dataset: {len(np.unique(np.hstack(x_valid)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nNumber of rows in test dataset: {x_test.shape[0]}')\n",
    "print(f'Number of columns in test dataset: {x_test.shape[1]}')\n",
    "print(f'Number of unique words in test dataset: {len(np.unique(np.hstack(x_test)))}')\n",
    "\n",
    "\n",
    "print('---'*20, f'\\nUnique Categories: {np.unique(y_train), np.unique(y_valid), np.unique(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdXPWuOmNEbh"
   },
   "source": [
    "### Print value of any one feature and it's label (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MGLEdeFmNZfR"
   },
   "source": [
    "Feature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKFyMa28zztL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    1,   14,   22,   16,   43,  530,\n",
       "        973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,   36,\n",
       "        256,    5,   25,  100,   43,  838,  112,   50,  670,    2,    9,\n",
       "         35,  480,  284,    5,  150,    4,  172,  112,  167,    2,  336,\n",
       "        385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,\n",
       "          4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,    4,\n",
       "       1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,\n",
       "         38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,\n",
       "         16,  626,   18,    2,    5,   62,  386,   12,    8,  316,    8,\n",
       "        106,    5,    4, 2223, 5244,   16,  480,   66, 3785,   33,    4,\n",
       "        130,   12,   16,   38,  619,    5,   25,  124,   51,   36,  135,\n",
       "         48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,\n",
       "          5,   14,  407,   16,   82,    2,    8,    4,  107,  117, 5952,\n",
       "         15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,\n",
       "        530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,\n",
       "        104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,\n",
       "        141,    6,  194, 7486,   18,    4,  226,   22,   21,  134,  476,\n",
       "         26,  480,    5,  144,   30, 5535,   18,   51,   36,   28,  224,\n",
       "         92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,\n",
       "         16,  283,    5,   16, 4472,  113,  103,   32,   15,   16, 5345,\n",
       "         19,  178,   32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_85Hqm0Nb1I"
   },
   "source": [
    "Label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FoehB5jNd1g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0cof4LSxNxuv"
   },
   "source": [
    "### Decode the feature value to get original sentence (4 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see the first review of the dataset, which is labeled as positive (1). \n",
    "The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. It replaces every unknown word with a â#â. It does this by using the get_word_index() function. We will print decoded sentence for 11th review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_oiAyPZOkJD"
   },
   "source": [
    "First, retrieve a dictionary that contains mapping of words to their index in the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NRgOD5S2Uuvd"
   },
   "source": [
    "Now use the dictionary to get the original words from the encodings, for a particular sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Clsk-yK8OtzD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a short while in the cell together they stumble upon a hiding place in the wall that contains an old # after # part of it they soon realise its magical powers and realise they may be able to use it to break through the prison walls br br black magic is a very interesting topic and i'm actually quite surprised that there aren't more films based on it as there's so much scope for things to do with it it's fair to say that # makes the best of it's # as despite it's # the film never actually feels restrained and manages to flow well throughout director eric # provides a great atmosphere for the film the fact that most of it takes place inside the central prison cell # that the film feels very claustrophobic and this immensely benefits the central idea of the prisoners wanting to use magic to break out of the cell it's very easy to get behind them it's often said that the unknown is the thing that really # people and this film proves that as the director # that we can never really be sure of exactly what is round the corner and this helps to ensure that # actually does manage to be quite frightening the film is memorable for a lot of reasons outside the central plot the characters are all very interesting in their own way and the fact that the book itself almost takes on its own character is very well done anyone worried that the film won't deliver by the end won't be disappointed either as the ending both makes sense and manages to be quite horrifying overall # is a truly great horror film and one of the best of the decade highly recommended viewing\n"
     ]
    }
   ],
   "source": [
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in X[10]] )\n",
    "print(decoded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the word index and then Create a key-value pair for word and word_id (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------ \n",
      " [(52012, \"hold's\"), (11310, 'comically'), (40833, 'localized'), (30571, 'disobeying'), (52013, \"'royale\"), (40834, \"harpo's\"), (52014, 'canet'), (19316, 'aileen'), (52015, 'acurately'), (52016, \"diplomat's\"), (25245, 'rickman'), (6749, 'arranged'), (52017, 'rumbustious'), (52018, 'familiarness'), (52019, \"spider'\"), (68807, 'hahahah'), (52020, \"wood'\"), (40836, 'transvestism'), (34705, \"hangin'\"), (2341, 'bringing'), (40837, 'seamier'), (34706, 'wooded'), (52021, 'bravora'), (16820, 'grueling'), (1639, 'wooden'), (16821, 'wednesday'), (52022, \"'prix\"), (34707, 'altagracia'), (52023, 'circuitry'), (11588, 'crotch'), (57769, 'busybody'), (52024, \"tart'n'tangy\"), (14132, 'burgade'), (52026, 'thrace'), (11041, \"tom's\"), (52028, 'snuggles'), (29117, 'francesco'), (52030, 'complainers'), (52128, 'templarios'), (40838, '272')]\n"
     ]
    }
   ],
   "source": [
    "def decode_review(x, y):\n",
    "  w2i = imdb.get_word_index()                                \n",
    "  w2i = {k:(v + 3) for k, v in w2i.items()}\n",
    "  w2i['<PAD>'] = 0\n",
    "  w2i['<START>'] = 1\n",
    "  w2i['<UNK>'] = 2\n",
    "  i2w = {i: w for w, i in w2i.items()}\n",
    "\n",
    "  ws = (' '.join(i2w[i] for i in x))\n",
    "  #print(f'Review: {ws}')\n",
    "  #print(f'Actual Sentiment: {y}')\n",
    "  return w2i, i2w\n",
    "\n",
    "w2i, i2w = decode_review(X[10], y[10]) # for 11th review\n",
    "\n",
    "# get first 50 key, value pairs from id to word dictionary\n",
    "print('---'*30, '\\n', list(islice(i2w.items(), 10, 50))) # for 11th review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLGABrJoVZe6"
   },
   "source": [
    "Get the sentiment for the above sentence\n",
    "- positive (1)\n",
    "- negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDyQGJT0Ve-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "print('Actual Sentiment:', y[10])\n",
    "#print(f'Actual Sentiment: {y[10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Sequential Model using Keras for the Sentiment Classification task (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BmCjr8miXIWB"
   },
   "source": [
    "### Define model (10 Marks)\n",
    "- Define a Sequential Model\n",
    "- Add Embedding layer\n",
    "  - Embedding layer turns positive integers into dense vectors of fixed size\n",
    "  - `tensorflow.keras` embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unique integer number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn LabelEncoder.\n",
    "  - Size of the vocabulary will be 10000\n",
    "  - Give dimension of the dense embedding as 100\n",
    "  - Length of input sequences should be 300\n",
    "- Add LSTM layer\n",
    "  - Pass value in `return_sequences` as True\n",
    "- Add a `TimeDistributed` layer with 100 Dense neurons\n",
    "- Add Flatten layer\n",
    "- Add Dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Embedding Layer Model\n",
    "\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "The embedding layer can be used at the start of a larger deep learning model.\n",
    "Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "Use the embedding layer to train our own word2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np5GxT1caFEq"
   },
   "outputs": [],
   "source": [
    "# Define a Sequential Model\n",
    "model = Sequential()\n",
    "# Add Embedding layer\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = 100, input_length = maxlen))#256 is dim of dense emb \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(256, 5, padding = 'same', activation = 'relu', strides = 1))\n",
    "model.add(Conv1D(128, 5, padding = 'same', activation = 'relu', strides = 1))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(Conv1D(64, 5, padding = 'same', activation = 'relu', strides = 1))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "\n",
    "#Add LSTM layer, Pass value in return_sequences as True\n",
    "model.add(LSTM(75,return_sequences=True))#,return_sequences=True\n",
    "#model.add(LSTM(75, return_sequences=False)) \n",
    "\n",
    "#Add a TimeDistributed layer with 100 Dense neurons\n",
    "model.add(TimeDistributed(Dense(100)))\n",
    "\n",
    "#Add a Flatten Layer\n",
    "model.add(Flatten()) # to covert to 3 dimensions into two dimensions\n",
    "\n",
    "#Add a Dense Layer\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hc4bknOobDby"
   },
   "source": [
    "### Compile the model (4 Marks)\n",
    "- Use Optimizer as Adam\n",
    "- Use Binary Crossentropy as loss\n",
    "- Use Accuracy as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sEzwazqbz3T"
   },
   "source": [
    "### Print model summary (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jw4RJ0CQbwFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 256)          128256    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 300, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 150, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 75, 75)            42000     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 75, 100)           7600      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7500)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7501      \n",
      "=================================================================\n",
      "Total params: 1,390,349\n",
      "Trainable params: 1,390,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bmkolKP4b-U6"
   },
   "source": [
    "### Fit the model (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRg3KFXLcAkk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/3\n",
      "32000/32000 [==============================] - 264s 8ms/step - loss: 0.3856 - accuracy: 0.7984 - val_loss: 0.2445 - val_accuracy: 0.8999\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24454, saving model to imdb_model.h5\n",
      "Epoch 2/3\n",
      "32000/32000 [==============================] - 260s 8ms/step - loss: 0.1901 - accuracy: 0.9276 - val_loss: 0.2523 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.24454\n",
      "Epoch 00002: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x245f1fa4408>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding callbacks\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 0)  \n",
    "mc = ModelCheckpoint('imdb_model.h5', monitor = 'val_loss', mode = 'min', save_best_only = True, verbose = 1)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data = (x_valid, y_valid), epochs = 3, batch_size = 64, verbose = True, callbacks = [es, mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Report the Accuracy of the model (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwLl54MXnkEA"
   },
   "source": [
    "### Evaluate model (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUqY-bD8RaDR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 16s 2ms/step\n",
      "Test accuracy: 90.07%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "print('Test accuracy: %.2f%%' % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2amr1tJn9Jz"
   },
   "source": [
    "### Predict on one sample (4 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wl4idfWR_A8E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90      5085\n",
      "           1       0.89      0.92      0.90      4915\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "print(f'Classification Report:\\n{classification_report(y_pred, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: 90%\n",
    "- F1-score: 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieve the output of each layer in Keras for a given single test sample from the trained model you built (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdbXlqq17W6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---------------------------------------- embedding_1 layer ---------------------------------------- \n",
      "\n",
      "[[[ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  [ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  [ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  ...\n",
      "  [-0.00481838 -0.03248607  0.00674376 ...  0.03830308 -0.02844862\n",
      "    0.00581411]\n",
      "  [-0.05558018 -0.04111317 -0.01504822 ...  0.01026597  0.04616052\n",
      "   -0.04665979]\n",
      "  [-0.01208614 -0.05179224  0.0576748  ...  0.07342824  0.01705603\n",
      "   -0.0771725 ]]]\n",
      "\n",
      " ---------------------------------------- dropout_1 layer ---------------------------------------- \n",
      "\n",
      "[[[ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  [ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  [ 0.00379257  0.00282501  0.0108014  ...  0.01597496 -0.00240109\n",
      "    0.03294621]\n",
      "  ...\n",
      "  [-0.00481838 -0.03248607  0.00674376 ...  0.03830308 -0.02844862\n",
      "    0.00581411]\n",
      "  [-0.05558018 -0.04111317 -0.01504822 ...  0.01026597  0.04616052\n",
      "   -0.04665979]\n",
      "  [-0.01208614 -0.05179224  0.0576748  ...  0.07342824  0.01705603\n",
      "   -0.0771725 ]]]\n",
      "\n",
      " ---------------------------------------- conv1d_1 layer ---------------------------------------- \n",
      "\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.07016066 0.12966482 ... 0.05847273 0.         0.        ]\n",
      "  [0.         0.         0.14926724 ... 0.13473761 0.01350219 0.        ]\n",
      "  [0.         0.         0.14868405 ... 0.17555979 0.0005721  0.        ]]]\n",
      "\n",
      " ---------------------------------------- conv1d_2 layer ---------------------------------------- \n",
      "\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.31643832]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "\n",
      " ---------------------------------------- max_pooling1d_1 layer ---------------------------------------- \n",
      "\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.31643832]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "\n",
      " ---------------------------------------- conv1d_3 layer ---------------------------------------- \n",
      "\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.09824398 0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.13809654 0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]]\n",
      "\n",
      " ---------------------------------------- max_pooling1d_2 layer ---------------------------------------- \n",
      "\n",
      "[[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.22240673]\n",
      "  [0.         0.         0.         ... 0.09824398 0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.13809654 0.         0.        ]]]\n",
      "\n",
      " ---------------------------------------- lstm_1 layer ---------------------------------------- \n",
      "\n",
      "[[[ 0.0011265  -0.00207529 -0.00041474 ...  0.00054068 -0.00163788\n",
      "    0.00050817]\n",
      "  [ 0.00204258 -0.00295761 -0.00052036 ...  0.00058646 -0.00267647\n",
      "    0.00089045]\n",
      "  [ 0.00268698 -0.00321319 -0.00051896 ...  0.00051663 -0.00340564\n",
      "    0.0012337 ]\n",
      "  ...\n",
      "  [ 0.12419351  0.13963479  0.15243287 ...  0.03883834 -0.19031192\n",
      "    0.08728523]\n",
      "  [ 0.14639053  0.12788288  0.13480268 ...  0.02456724 -0.14125822\n",
      "    0.04329088]\n",
      "  [ 0.15293562  0.10947514  0.12821832 ...  0.01362224 -0.11446539\n",
      "    0.0243962 ]]]\n",
      "\n",
      " ---------------------------------------- time_distributed_1 layer ---------------------------------------- \n",
      "\n",
      "[[[-0.00285294 -0.0059776  -0.00935079 ... -0.00663019  0.0032465\n",
      "    0.00249969]\n",
      "  [-0.003579   -0.00654547 -0.01036412 ... -0.00832258  0.00504103\n",
      "    0.00202125]\n",
      "  [-0.00453567 -0.00690271 -0.01108072 ... -0.00942035  0.0061704\n",
      "    0.00164155]\n",
      "  ...\n",
      "  [-0.1450458  -0.30459958  0.22347532 ...  0.07353944 -0.08193879\n",
      "    0.17615439]\n",
      "  [-0.14842223 -0.3000735   0.23019579 ...  0.08668376 -0.08427264\n",
      "    0.13456942]\n",
      "  [-0.1453541  -0.28755116  0.21787132 ...  0.09630489 -0.08701491\n",
      "    0.11919115]]]\n",
      "\n",
      " ---------------------------------------- flatten_1 layer ---------------------------------------- \n",
      "\n",
      "[[-0.00285294 -0.0059776  -0.00935079 ...  0.09630489 -0.08701491\n",
      "   0.11919115]]\n",
      "\n",
      " ---------------------------------------- dense_2 layer ---------------------------------------- \n",
      "\n",
      "[[9.892191e-06]]\n"
     ]
    }
   ],
   "source": [
    "sample_x_test = x_test[np.random.randint(10000)]\n",
    "for layer in model.layers:\n",
    "\n",
    "    model_layer = Model(inputs = model.input, outputs = model.get_layer(layer.name).output)\n",
    "    output = model_layer.predict(sample_x_test.reshape(1,-1))\n",
    "    print('\\n','--'*20, layer.name, 'layer', '--'*20, '\\n')\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-1:\n",
      "Actual sentiment: 1\n",
      "Predicted sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "decode_review(x_test[10], y_test[10])\n",
    "print('Test-1:')\n",
    "print(f'Actual sentiment: {y_test[10]}')\n",
    "print(f'Predicted sentiment: {y_pred[10][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-2:\n",
      "Actual sentiment: 0\n",
      "Predicted sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "print('Test-2:')\n",
    "print(f'Actual sentiment: {y_test[200]}')\n",
    "print(f'Predicted sentiment: {y_pred[200][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment classification task on the IMDB dataset is performed using word embedding. Built the sequential model using LSTM layer along with convolution, max pooling, flatten and Dense layers. \n",
    "\n",
    "On test dataset we could achieve good accuracy score.\n",
    "- Accuracy: 90%\n",
    "- F1-score: 90%\n",
    "- Valedation Loss: 0.25\n",
    "\n",
    "Tested the sample predictions and got the matching output with the target sentiments."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Questions - Project 1 - Sequential Models in NLP - Sentiment Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
